{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('virtual_env')",
   "metadata": {
    "interpreter": {
     "hash": "ae520ac60bc58bf887efe3d8b884bb1031c488da0214529d17ecae648d5e994e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import zipfile\n",
    "import tempfile\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_puf_data(first_week, last_week):\n",
    "    '''\n",
    "    download puf files for the given weeks and concatenate the datasets\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(first_week,last_week+1):\n",
    "        y = 2020\n",
    "        if i>=22:\n",
    "            y=2021\n",
    "        file_str = \"pulse{}_puf_{}.csv\".format(y,i)\n",
    "        url_str = \"https://www2.census.gov/programs-surveys/demo/datasets/hhp/{yr}/wk{w}/HPS_Week{w}_PUF_CSV.zip\".format(yr=y,w=i)\n",
    "        week_df = download_url(url_str, file_str, y, i)\n",
    "        df = pd.concat([df,week_df])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def download_url(url_str,file_str, year, week, chunk_size=128):\n",
    "    '''\n",
    "    download puf, avoid intermediate save to disk\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    r = requests.get(url_str)\n",
    "    print(r)\n",
    "    #temp directory rather than file to hopefully accomodate windows users\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        f_name = os.path.join(td, 'week_{}'.format(week))\n",
    "        with open(f_name, 'wb') as fh:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                fh.write(chunk)\n",
    "        zf = zipfile.ZipFile(f_name)\n",
    "        df = pd.read_csv(zf.open(file_str))\n",
    "        df1 = pd.read_csv(zf.open(\"pulse{}_repwgt_puf_{}.csv\".format(year,week)))\n",
    "        zf.close()\n",
    "        r.close()\n",
    "\n",
    "    return pd.merge(df,df1,on=[\"SCRAM\",\"WEEK\"],how=\"inner\")\n",
    "\n",
    "\n",
    "def get_std_err(df, weight):\n",
    "    #make 1d array of weight col\n",
    "    obs_wgts = df[weight].to_numpy().reshape(len(df),1)\n",
    "    \n",
    "    #make 80d array of replicate weights\n",
    "    rep_wgts = df[[i for i in df.columns if weight in i and not i == weight]].to_numpy()\n",
    "    \n",
    "    #return standard error of estimate\n",
    "    return np.sqrt((np.sum(np.square(rep_wgts-obs_wgts),axis=1)*(4/80)))\n",
    "\n",
    "\n",
    "def freq_crosstab(df, col_list, weight, critical_val=1):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    df1 = df[-df[col_list].isin([-99,-88]).any(axis=1)].copy()\n",
    "    pt_estimates = df1.groupby(col_list, as_index=True)[[i for i in df1.columns if weight in i]].agg('sum')\n",
    "    pt_estimates['std_err'] = get_std_err(pt_estimates, weight)\n",
    "    pt_estimates['mrgn_err'] = pt_estimates.std_err * critical_val\n",
    "\n",
    "    return pt_estimates[[weight, 'std_err','mrgn_err']] #.reset_index()\n",
    "\n",
    "def full_crosstab(df, col_list, weight, proportion_level, critical_val=1):\n",
    "    rv = freq_crosstab(df, col_list, weight, critical_val)\n",
    "    rv['proportions'] = rv[weight].groupby(proportion_level).apply(lambda x: x / float(x.sum()))\n",
    "    return rv.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "core_df = get_puf_data(13,22)\n",
    "q_map_df = pd.read_csv('data/question_mapping.csv')\n",
    "r_map_df = pd.read_csv('data/response_mapping.csv')"
   ]
  },
  {
   "source": [
    "## Tie Out Housing 1b Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df['rent_check'] = core_df.apply(lambda x: 4 if x.TENURE==4 else x.RENTCUR, axis=1) \n",
    "freq_crosstab(df, ['EST_MSA', 'EGENDER', 'RENTCUR'], 'PWEIGHT', critical_val=1.645).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      WEEK    metro   race     rent  PWEIGHT_x  std_err_x  mrgn_err_x  \\\n",
       "0       13  Atlanta  Asian   behind    5923.87    3624.72     5962.66   \n",
       "1       13  Atlanta  Asian  current   20534.83    7041.32    11582.98   \n",
       "2       13  Atlanta  Other   behind    8375.40    5765.38     9484.05   \n",
       "3       13  Atlanta  Other  current   18352.92    6439.09    10592.30   \n",
       "4       13  Atlanta  black   behind   84377.39   25286.83    41596.84   \n",
       "...    ...      ...    ...      ...        ...        ...         ...   \n",
       "1172    22  Seattle  Other  current  103349.20   28112.04    46244.30   \n",
       "1173    22  Seattle  black   behind   13463.14    9451.65    15547.96   \n",
       "1174    22  Seattle  black  current   63321.88   29651.54    48776.79   \n",
       "1175    22  Seattle  white   behind   72582.99   15875.70    26115.53   \n",
       "1176    22  Seattle  white  current  478989.17   38147.29    62752.30   \n",
       "\n",
       "      proportions_x  PWEIGHT_y  std_err_y  mrgn_err_y  proportions_y  \n",
       "0              0.22   26458.70    7205.60    11853.21           0.03  \n",
       "1              0.78   26458.70    7205.60    11853.21           0.03  \n",
       "2              0.31   26728.32   10129.81    16663.54           0.03  \n",
       "3              0.69   26728.32   10129.81    16663.54           0.03  \n",
       "4              0.20  418937.82   54846.83    90223.04           0.48  \n",
       "...             ...        ...        ...         ...            ...  \n",
       "1172           0.86  120849.01   29641.23    48759.83           0.14  \n",
       "1173           0.18   76785.02   31119.11    51190.94           0.09  \n",
       "1174           0.82   76785.02   31119.11    51190.94           0.09  \n",
       "1175           0.13  551572.16   40315.17    66318.46           0.63  \n",
       "1176           0.87  551572.16   40315.17    66318.46           0.63  \n",
       "\n",
       "[1177 rows x 12 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>WEEK</th>\n      <th>metro</th>\n      <th>race</th>\n      <th>rent</th>\n      <th>PWEIGHT_x</th>\n      <th>std_err_x</th>\n      <th>mrgn_err_x</th>\n      <th>proportions_x</th>\n      <th>PWEIGHT_y</th>\n      <th>std_err_y</th>\n      <th>mrgn_err_y</th>\n      <th>proportions_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13</td>\n      <td>Atlanta</td>\n      <td>Asian</td>\n      <td>behind</td>\n      <td>5923.87</td>\n      <td>3624.72</td>\n      <td>5962.66</td>\n      <td>0.22</td>\n      <td>26458.70</td>\n      <td>7205.60</td>\n      <td>11853.21</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13</td>\n      <td>Atlanta</td>\n      <td>Asian</td>\n      <td>current</td>\n      <td>20534.83</td>\n      <td>7041.32</td>\n      <td>11582.98</td>\n      <td>0.78</td>\n      <td>26458.70</td>\n      <td>7205.60</td>\n      <td>11853.21</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>Atlanta</td>\n      <td>Other</td>\n      <td>behind</td>\n      <td>8375.40</td>\n      <td>5765.38</td>\n      <td>9484.05</td>\n      <td>0.31</td>\n      <td>26728.32</td>\n      <td>10129.81</td>\n      <td>16663.54</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13</td>\n      <td>Atlanta</td>\n      <td>Other</td>\n      <td>current</td>\n      <td>18352.92</td>\n      <td>6439.09</td>\n      <td>10592.30</td>\n      <td>0.69</td>\n      <td>26728.32</td>\n      <td>10129.81</td>\n      <td>16663.54</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13</td>\n      <td>Atlanta</td>\n      <td>black</td>\n      <td>behind</td>\n      <td>84377.39</td>\n      <td>25286.83</td>\n      <td>41596.84</td>\n      <td>0.20</td>\n      <td>418937.82</td>\n      <td>54846.83</td>\n      <td>90223.04</td>\n      <td>0.48</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1172</th>\n      <td>22</td>\n      <td>Seattle</td>\n      <td>Other</td>\n      <td>current</td>\n      <td>103349.20</td>\n      <td>28112.04</td>\n      <td>46244.30</td>\n      <td>0.86</td>\n      <td>120849.01</td>\n      <td>29641.23</td>\n      <td>48759.83</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>1173</th>\n      <td>22</td>\n      <td>Seattle</td>\n      <td>black</td>\n      <td>behind</td>\n      <td>13463.14</td>\n      <td>9451.65</td>\n      <td>15547.96</td>\n      <td>0.18</td>\n      <td>76785.02</td>\n      <td>31119.11</td>\n      <td>51190.94</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>1174</th>\n      <td>22</td>\n      <td>Seattle</td>\n      <td>black</td>\n      <td>current</td>\n      <td>63321.88</td>\n      <td>29651.54</td>\n      <td>48776.79</td>\n      <td>0.82</td>\n      <td>76785.02</td>\n      <td>31119.11</td>\n      <td>51190.94</td>\n      <td>0.09</td>\n    </tr>\n    <tr>\n      <th>1175</th>\n      <td>22</td>\n      <td>Seattle</td>\n      <td>white</td>\n      <td>behind</td>\n      <td>72582.99</td>\n      <td>15875.70</td>\n      <td>26115.53</td>\n      <td>0.13</td>\n      <td>551572.16</td>\n      <td>40315.17</td>\n      <td>66318.46</td>\n      <td>0.63</td>\n    </tr>\n    <tr>\n      <th>1176</th>\n      <td>22</td>\n      <td>Seattle</td>\n      <td>white</td>\n      <td>current</td>\n      <td>478989.17</td>\n      <td>38147.29</td>\n      <td>62752.30</td>\n      <td>0.87</td>\n      <td>551572.16</td>\n      <td>40315.17</td>\n      <td>66318.46</td>\n      <td>0.63</td>\n    </tr>\n  </tbody>\n</table>\n<p>1177 rows Ã— 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "source": [
    "states = {1:'Alabama',2:'Alaska',4:'Arizona',\n",
    "    5:'Arkansas',\n",
    "    6:'California',\n",
    "    8:'Colorado',\n",
    "    9:'Connecticut',\n",
    "    10:'Delaware',\n",
    "    11:'District of Columbia',\n",
    "    12:'Florida',\n",
    "    13:'Georgia',\n",
    "    15:'Hawaii',\n",
    "    16:'Idaho',\n",
    "    17:'Illinois',\n",
    "    18:'Indiana',\n",
    "    19:'Iowa',\n",
    "    20:'Kansas',\n",
    "    21:'Kentucky',\n",
    "    22:'Louisiana',\n",
    "    23:'Maine',\n",
    "    24:'Maryland',\n",
    "    25:'Massachusetts',\n",
    "    26:'Michigan',\n",
    "    27:'Minnesota',\n",
    "    28:'Mississippi',\n",
    "    29:'Missouri',\n",
    "    30:'Montana',\n",
    "    31:'Nebraska',\n",
    "    32:'Nevada',\n",
    "    33:'New Hampshire',\n",
    "    34:'New Jersey',\n",
    "    35:'New Mexico',\n",
    "    36:'New York',\n",
    "    37:'North Carolina',\n",
    "    38:'North Dakota',\n",
    "    39:'Ohio',\n",
    "    40:'Oklahoma',\n",
    "    41:'Oregon',\n",
    "    42:'Pennsylvania',\n",
    "    44:'Rhode Island',\n",
    "    45:'South Carolina',\n",
    "    46:'South Dakota',\n",
    "    47:'Tennessee',\n",
    "    48:'Texas',\n",
    "    49:'Utah',\n",
    "    50:'Vermont',\n",
    "    51:'Virginia',\n",
    "    53:'Washington',\n",
    "    54:'West Virginia',\n",
    "    55:'Wisconsin',\n",
    "    56:'Wyoming'}\n",
    "regions = {1:'NE',2:'S',3:'MW',4:'W'}\n",
    "metros = {35620:'New York',\n",
    "31080:'Los Angeles',\n",
    "16980:'Chicago',\n",
    "19100:'Dallas',\n",
    "26420:'Houston',\n",
    "47900:'DC',\n",
    "33100:'Miami',\n",
    "37980:'Philadelphia',\n",
    "12060:'Atlanta',\n",
    "38060:'Phoenix',\n",
    "14460:'Boston',\n",
    "41860:'San Francisco',\n",
    "40140:'Riverside CA',\n",
    "19820:'Detroit',\n",
    "42660:'Seattle'}\n",
    "races = {1:'white',2:'black',3:'Asian',4:'Other'}\n",
    "rents = {1:'current',2:'behind'}\n",
    "df = core_df.copy()\n",
    "df1 = core_df.copy()\n",
    "df = df[-df.EST_MSA.isna()]\n",
    "df = df[df.RRACE>0]\n",
    "df = df[df.RENTCUR>0]\n",
    "df1 = df1[df1.RRACE>0]\n",
    "df1 = df1[df1.RENTCUR>0]\n",
    "df['metro'] = df.apply(lambda x: metros[x.EST_MSA],axis=1)\n",
    "df['race'] = df.apply(lambda x: races[x.RRACE],axis=1)\n",
    "df['rent'] = df.apply(lambda x: rents[x.RENTCUR],axis=1)\n",
    "df1['state'] = df1.apply(lambda x: states[x.EST_ST],axis=1)\n",
    "df1['region'] = df1.apply(lambda x: regions[x.REGION],axis=1) \n",
    "df1['race'] = df1.apply(lambda x: races[x.RRACE],axis=1)\n",
    "df1['rent'] = df1.apply(lambda x: rents[x.RENTCUR],axis=1)\n",
    "x = full_crosstab(df, ['WEEK','metro','race','rent'], 'PWEIGHT', ['WEEK','metro','race'], critical_val=1.645).round(2)\n",
    "y = full_crosstab(df, ['WEEK','metro','race'], 'PWEIGHT', ['WEEK','metro'], critical_val=1.645).round(2)\n",
    "rv = x.merge(y,'left',['WEEK','metro','race'])\n",
    "rv\n",
    "#rv.to_csv('crosstab1.csv')\n",
    "#rv = freq_crosstab(df, ['EST_MSA','RENTCUR'], 'PWEIGHT', critical_val=1.645)\n",
    "#rv['proportions'] = rv['PWEIGHT'].groupby(['EST_MSA']).apply(lambda x: x / float(x.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv.to_csv('crosstab2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     metro   race     rent    PWEIGHT   std_err  mrgn_err  proportions\n",
       "0  Atlanta  Asian   behind    5923.87   3624.72   5962.66         0.22\n",
       "1  Atlanta  Asian  current   20534.83   7041.32  11582.98         0.78\n",
       "2  Atlanta  Other   behind    8375.40   5765.38   9484.05         0.31\n",
       "3  Atlanta  Other  current   18352.92   6439.09  10592.30         0.69\n",
       "4  Atlanta  black   behind   84377.39  25286.83  41596.84         0.20\n",
       "5  Atlanta  black  current  334560.43  52043.74  85611.95         0.80\n",
       "6  Atlanta  white   behind   32461.67  13918.24  22895.50         0.08\n",
       "7  Atlanta  white  current  361599.05  46427.68  76373.54         0.92"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>metro</th>\n      <th>race</th>\n      <th>rent</th>\n      <th>PWEIGHT</th>\n      <th>std_err</th>\n      <th>mrgn_err</th>\n      <th>proportions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Atlanta</td>\n      <td>Asian</td>\n      <td>behind</td>\n      <td>5923.87</td>\n      <td>3624.72</td>\n      <td>5962.66</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Atlanta</td>\n      <td>Asian</td>\n      <td>current</td>\n      <td>20534.83</td>\n      <td>7041.32</td>\n      <td>11582.98</td>\n      <td>0.78</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Atlanta</td>\n      <td>Other</td>\n      <td>behind</td>\n      <td>8375.40</td>\n      <td>5765.38</td>\n      <td>9484.05</td>\n      <td>0.31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Atlanta</td>\n      <td>Other</td>\n      <td>current</td>\n      <td>18352.92</td>\n      <td>6439.09</td>\n      <td>10592.30</td>\n      <td>0.69</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Atlanta</td>\n      <td>black</td>\n      <td>behind</td>\n      <td>84377.39</td>\n      <td>25286.83</td>\n      <td>41596.84</td>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Atlanta</td>\n      <td>black</td>\n      <td>current</td>\n      <td>334560.43</td>\n      <td>52043.74</td>\n      <td>85611.95</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Atlanta</td>\n      <td>white</td>\n      <td>behind</td>\n      <td>32461.67</td>\n      <td>13918.24</td>\n      <td>22895.50</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Atlanta</td>\n      <td>white</td>\n      <td>current</td>\n      <td>361599.05</td>\n      <td>46427.68</td>\n      <td>76373.54</td>\n      <td>0.92</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "#full_crosstab(df, ['WEEK','metro'], 'PWEIGHT', ['WEEK','metro'], critical_val=1.645).round(2)\n",
    "df1 = df[df.WEEK==13].copy()\n",
    "df1 = df1[df1.metro=='Atlanta']\n",
    "full_crosstab(df1, ['metro','race','rent'], 'PWEIGHT', ['metro','race'], critical_val=1.645).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Alabama', 'Iowa', 'North Carolina', 'South Dakota', 'Virginia',\n",
       "       'Illinois', 'Minnesota', 'New York', 'Ohio', 'Washington',\n",
       "       'Connecticut', 'Indiana', 'New Mexico', 'Oregon', 'Georgia',\n",
       "       'Louisiana', 'Maryland', 'Michigan', 'Pennsylvania', 'Wisconsin',\n",
       "       'Alaska', 'District of Columbia', 'Nevada', 'Tennessee',\n",
       "       'Mississippi', 'Nebraska', 'Oklahoma', 'Rhode Island',\n",
       "       'South Carolina', 'Missouri', 'Texas', 'Colorado', 'Delaware',\n",
       "       'Florida', 'Kentucky', 'Massachusetts', 'Vermont', 'California',\n",
       "       'Kansas', 'New Jersey', 'Utah'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 217
    }
   ],
   "source": [
    "detail = full_crosstab(df1, ['WEEK','state','race','rent'], 'PWEIGHT', ['WEEK','state','race'], critical_val=1.645).round(2)\n",
    "top = full_crosstab(df1, ['WEEK','state','race'], 'PWEIGHT', ['WEEK','state'], critical_val=1.645).round(2)\n",
    "rv = detail.merge(top,'left',['WEEK','state','race'],suffixes=('_full','_demo'))\n",
    "y = rv[rv.rent=='behind']\n",
    "y_2 = y[y.proportions_full>0.4]\n",
    "y_3 = y_2[y_2.PWEIGHT_full>y_2.mrgn_err_full]\n",
    "y_3.state.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     WEEK region   race    rent  PWEIGHT_full  std_err_full  mrgn_err_full  \\\n",
       "172    18     NE  black  behind     857991.48     189038.01      310967.52   \n",
       "292    22     MW  black  behind     733948.69     156260.04      257047.76   \n",
       "298    22     NE  Other  behind     396776.44     110644.16      182009.64   \n",
       "\n",
       "     proportions_full  PWEIGHT_demo  std_err_demo  mrgn_err_demo  \\\n",
       "172              0.44    1928187.96     195373.94      321390.14   \n",
       "292              0.50    1479393.27     156888.39      258081.41   \n",
       "298              0.57     701300.10     117815.62      193806.69   \n",
       "\n",
       "     proportions_demo  \n",
       "172              0.20  \n",
       "292              0.17  \n",
       "298              0.08  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>WEEK</th>\n      <th>region</th>\n      <th>race</th>\n      <th>rent</th>\n      <th>PWEIGHT_full</th>\n      <th>std_err_full</th>\n      <th>mrgn_err_full</th>\n      <th>proportions_full</th>\n      <th>PWEIGHT_demo</th>\n      <th>std_err_demo</th>\n      <th>mrgn_err_demo</th>\n      <th>proportions_demo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>172</th>\n      <td>18</td>\n      <td>NE</td>\n      <td>black</td>\n      <td>behind</td>\n      <td>857991.48</td>\n      <td>189038.01</td>\n      <td>310967.52</td>\n      <td>0.44</td>\n      <td>1928187.96</td>\n      <td>195373.94</td>\n      <td>321390.14</td>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>292</th>\n      <td>22</td>\n      <td>MW</td>\n      <td>black</td>\n      <td>behind</td>\n      <td>733948.69</td>\n      <td>156260.04</td>\n      <td>257047.76</td>\n      <td>0.50</td>\n      <td>1479393.27</td>\n      <td>156888.39</td>\n      <td>258081.41</td>\n      <td>0.17</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>22</td>\n      <td>NE</td>\n      <td>Other</td>\n      <td>behind</td>\n      <td>396776.44</td>\n      <td>110644.16</td>\n      <td>182009.64</td>\n      <td>0.57</td>\n      <td>701300.10</td>\n      <td>117815.62</td>\n      <td>193806.69</td>\n      <td>0.08</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 209
    }
   ],
   "source": [
    "detail_r = full_crosstab(df1, ['WEEK','region','race','rent'], 'PWEIGHT', ['WEEK','region','race'], critical_val=1.645).round(2)\n",
    "top_r = full_crosstab(df1, ['WEEK','region','race'], 'PWEIGHT', ['WEEK','region'], critical_val=1.645).round(2)\n",
    "rv_r = detail_r.merge(top_r,'left',['WEEK','region','race'],suffixes=('_full','_demo'))\n",
    "x = rv_r[rv_r.rent=='behind']\n",
    "x[x.proportions_full>0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}